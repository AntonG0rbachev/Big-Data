{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOFmI5J8x_fL"
      },
      "source": [
        "# Пример из исследования: подготовка данных данных ссуд для самообслуживания BI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c-wxzcUx_fL"
      },
      "source": [
        "<b> замечание: </b> для этого тематического исследования мы будем использовать Hortonbox (версия была 2.3.2 на момент написания этой главы)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSmuCDAtx_fM"
      },
      "source": [
        "### Чем заняться на Hortonbox до работы системы:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rujQu9azx_fM"
      },
      "source": [
        "<p> <b> замечание: </b> вы можете подключиться к Hortonbox через PuTTY, это обычно проще, чем работа над самой виртуальной машиной </p>\n",
        "<p> yum -y установить python -pip </p>\n",
        "<p> PIP установить git+https: //github.com/davycielen/pywebhdfs.git -облегчить </p>\n",
        "<p> <b> замечание: </b> Этот пользовательский репозиторий представляет собой вилку pywebhdfs. Pywebhdfs был сломлен на момент написания статьи. Реквизит для его исправления был выпущен, но пока основной репо снова не сработает должным образом, используйте пользовательский, который мы создали </p>\n",
        "<p> pip установить панды </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCYz3Q9mx_fM"
      },
      "source": [
        "<p> <b> замечание: </b> все следующее <b> код </b> необходимо запустить <b> в саме Hortonbox </b> </p>\n",
        "<p> Чтобы сделать это, откройте сессию замазки и выпустите команду «Pyspark», это откроет интерпретатор Python </p>\n",
        "<p> Если все прошло уже задолго до этого, этот переводчик также должен иметь библиотеки Pandas и pywebhdfs (или адаптированных Pywebhdfs) </p>\n",
        "<p> Весь следующий код должен быть запущен в этом сеансе Pyspark (вы можете просто скопировать вставку его кусочком) </p>\n",
        "<p> Существуют другие параметры, такие как копирование кода в файл .py и запуск его там, другой вариант - ноутбук Zeppelin </p>\n",
        "<p> Zeppelin установлен на Hortonbox, но не был достаточно зрелым, чтобы полагаться на момент написания. </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixWPMaUTx_fM"
      },
      "source": [
        "## Часть 1 процесса науки о данных: цель исследования"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYI-X2PKx_fN"
      },
      "source": [
        "<b> Основная цель обучения: </b> Использование технологий больших данных для подготовки данных для <b> самообслуживания Bi </b>. В этом тематическом исследовании мы не пойдем в построение модели, а вместо этого тратим больше времени на подготовку данных, чтобы позволить другим людям исследовать его и в конечном итоге создавать отчеты, используя подготовленные данные. Концепция BI самообслуживания позволяет конечным пользователям находить свои собственные идеи и часто применяется, когда компания очень ориентирована на данные, но не хватает ученых данных, чтобы предоставить информацию. В главе 9 также будет представлена ​​тематическое исследование с некоторым более вниманием на применении уровня панели панели для самообслуживания BI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGrkwp_Rx_fN"
      },
      "source": [
        "## Часть 2 процесса данных: поиск данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKTQjD5Mx_fN"
      },
      "source": [
        "### Загрузите данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OxAQwbly5Rx"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from StringIO import StringIO # # для Python 2\n",
        "except ImportError:\n",
        "    from io import StringIO # # для Python 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "executionInfo": {
          "elapsed": 297,
          "status": "ok",
          "timestamp": 1733938623265,
          "user": {
            "displayName": "Евгений Калайдин",
            "userId": "02661115068327008026"
          },
          "user_tz": -180
        },
        "id": "uCm3JmlAx_fN"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "# импортировать Stringio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 7374,
          "status": "ok",
          "timestamp": 1733939107164,
          "user": {
            "displayName": "Евгений Калайдин",
            "userId": "02661115068327008026"
          },
          "user_tz": -180
        },
        "id": "2qp-C8lH0eY1",
        "outputId": "08102dff-3561-406a-96b0-cf443d4a13b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'resources.lendingclub.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "source = requests.get(\"https://resources.lendingclub.com/LoanStats3d.csv.zip\", verify=False) # А"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9oMx4Qq30yFV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ]
        }
      ],
      "source": [
        "print(source.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "collapsed": true,
        "executionInfo": {
          "elapsed": 373,
          "status": "error",
          "timestamp": 1733939168661,
          "user": {
            "displayName": "Евгений Калайдин",
            "userId": "02661115068327008026"
          },
          "user_tz": -180
        },
        "id": "od7S1AYhx_fN",
        "outputId": "576bc899-2c73-4acc-aea7-f8e90c816bdc"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "initial_value must be str or None, not Response",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7c06cb18d19c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstringio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0munzipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstringio\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: initial_value must be str or None, not Response"
          ]
        }
      ],
      "source": [
        "stringio = StringIO(source) # Беременный\n",
        "unzipped = zipfile.ZipFile(stringio) # В"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kyURK9_ux_fN"
      },
      "outputs": [],
      "source": [
        "# Загрузите данные из кредитного клуба. Это HTTPS, так что это должно быть проверено, но мы не будем беспокоиться (проверьте = false)\n",
        "# B создает виртуальный файл.\n",
        "# C Разанипируйте данные"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1uSk8r1x_fO"
      },
      "source": [
        "### Переместить данные в Hadoop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "T00-2Uutx_fO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pywebhdfs.webhdfs import PyWebHdfsClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hNAl4tbx_fO"
      },
      "outputs": [],
      "source": [
        "subselection_csv = pd.read_csv(unzipped.open('LoanStats3d.csv'),skiprows=1,skipfooter=2,engine='python') # А\n",
        "stored_csv = subselection_csv.to_csv('./stored_csv.csv') # Беременный"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0gTvHOuWx_fO"
      },
      "outputs": [],
      "source": [
        "hdfs = PyWebHdfsClient(user_name=\"hdfs\",port=50070,host=\"sandbox\")# В\n",
        "hdfs.make_dir('chapter5')# Дюймовый\n",
        "with open('./stored_csv.csv') as file_data: # Эн\n",
        "        hdfs.create_file('chapter5/LoanStats3d.csv',file_data, overwrite=True)# Фон"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7H9OCB7Dx_fO"
      },
      "outputs": [],
      "source": [
        "# A Сделайте некоторую предварительную очистку данных с помощью Pandas: удалить верхний ряд и нижние 2 ряда, потому что они бесполезны для нас. Просто открытие оригинального файла покажет вам это.\n",
        "# B Храните его локально, потому что нам нужно перенести его в файловую систему Hadoop\n",
        "# C Подключитесь к песочнице Hadoop\n",
        "# D Создать папку с названием «Глава 5» на файловой системе Hadoop\n",
        "# E Откройте локально хранящийся CSV\n",
        "# F Создайте файл .csv на файловой системе Hadoop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qZHXJcuZx_fO"
      },
      "outputs": [],
      "source": [
        "print hdfs.get_file_dir_status('chapter5/LoanStats3d.csv')# А"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_j6w018Cx_fO"
      },
      "outputs": [],
      "source": [
        "# Распечатать статус файла"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5Ki9ucpDx_fO"
      },
      "outputs": [],
      "source": [
        "# hdfs.delete_file_dir ('Глава 5/Loanstats3d.csv', recurisive = true) #A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hYS_1gndx_fO"
      },
      "outputs": [],
      "source": [
        "# Код на случай, если вы хотите удалить файл"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQUpsgRSx_fO"
      },
      "source": [
        "## Часть 3 процесса данных: подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCl60y0Kx_fO"
      },
      "source": [
        "### Очистка данных Apache Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nwQPDYgVx_fO"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext # А\n",
        "from pyspark.sql import HiveContext # Беременный\n",
        "# sc = sparkcontext ()#c\n",
        "sqlContext = HiveContext(sc)# Дюймовый\n",
        "data = sc.textFile(\"/chapter5/LoanStats3d.csv\") # Эн\n",
        "parts = data.map(lambda r:r.split(',')) # Фон\n",
        "firstline = parts.first() # Глин\n",
        "datalines = parts.filter(lambda x:x != firstline)# ЧАС\n",
        "def cleans(row): # я\n",
        "        row[7] = str(float(row[7][:-1])/100)# Дж\n",
        "        return [s.encode('utf8').replace(r\"_\",\" \").lower() for s in row]# K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LrqGc4Drx_fP"
      },
      "outputs": [],
      "source": [
        "datalines = datalines.map(lambda x: cleans(x))# Л"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JP8Ol0Glx_fP"
      },
      "outputs": [],
      "source": [
        "# Контекст импорта искры -> не обязательно при работе непосредственно в Pyspark!\n",
        "# B Контекст импорта улей\n",
        "# C В сеансе Pyspark SparkContext автоматически присутствует. В других случаях (ноутбук Zeppelin) вам нужно будет\n",
        "# Создайте это явно\n",
        "# D Создать HiveContext\n",
        "# E загрузка в наборе данных из каталога Hadoop\n",
        "# F Разделите набор данных с разделителем komma (,). Это концом разделителя строки для этого файла\n",
        "# G Возьмите первую строку\n",
        "# H Храните все линии, но первая строка. Потому что первая строка - это просто имена переменных\n",
        "# I Функция очистки будет использовать силу искры для очистки данных. Ввод этой функции будет линейкой данных.\n",
        "# J столбец 8 (index = 7) имеет % форматированных чисел. Нам не нужен этот знак «%».\n",
        "# K кодирует все в UTF8, замените подчеркивание на пробелы и все строчные.\n",
        "# L Экскурация линии очистки данных по линии"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eX_D3oTx_fP"
      },
      "source": [
        "### Сохранение данных в Hive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gIVTxFOax_fP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import * # А\n",
        "fields = [StructField(field_name,StringType(),True) for field_name in firstline] # Беременный\n",
        "schema = StructType(fields) # В\n",
        "schemaLoans = sqlContext.createDataFrame(datalines, schema) # Дюймовый\n",
        "schemaLoans.registerTempTable(\"loans\") # Эн\n",
        "\n",
        "sqlContext.sql(\"drop table if exists LoansByTitle\") # Фон\n",
        "sql = '''create table LoansByTitle stored as parquet as select title, count(1) as number from loans group by title order by number desc'''# Фон\n",
        "sqlContext.sql(sql)# Фон\n",
        "\n",
        "sqlContext.sql('drop table if exists raw') # Глин\n",
        "sql = '''create table raw stored as parquet as select title, emp_title,grade,home_ownership,int_rate,recoveries,collection_recovery_fee,loan_amnt,term from loans'''# Глин\n",
        "sqlContext.sql(sql)# Глин"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3TEawwP7x_fP"
      },
      "outputs": [],
      "source": [
        "# Импорт типов данных SQL\n",
        "# B Создание метаданных: функция Spark SQL Structfield представляет поле в структуре.\n",
        "# Объект Structfield состоит из трех полей: name (a String), DataType (DataType) и «Nullable» (логический).\n",
        "# Поле имени является названием Structfield.\n",
        "# Поле DataType определяет тип данных Structfield.\n",
        "# Поле Nullable указывает, могут ли значения структуры не содержать значений.\n",
        "# C Functype Функция создает схему данных. Объект structtype требует списка структуры в качестве Imput.\n",
        "# D Создайте DataFrame из Data (DataLines) и Dataschema (Schema)\n",
        "# E Зарегистрируйте его в виде таблицы, называемого кредитами\n",
        "# F Drop Table (в случае, если она уже существует), суммируйте и храните в Hive. Loansbytitle представляет сумму кредитов по названию работы\n",
        "# G Drop Table (в случае, если она уже существует) и сохранить подмножество необработанных данных в Hive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePE6EToux_fP"
      },
      "source": [
        "## Часть 4 процесса данных: исследование данных и часть 6: презентация для конечного пользователя"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xACzijkx_fP"
      },
      "source": [
        "Разведка данных, часть этого тематического исследования, будет сделана в Qlick. В этом тематическом исследовании нет реальной фазы моделирования. Поскольку Qlick позволяет вам создавать панель панели и отчеты, часть 4 и 6 образуют перекрытие."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "",
      "version": ""
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}